{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64d5add-4ef6-4b49-a95e-d1228882f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import re\n",
    "import ast\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import pymorphy2\n",
    "    _MORPH = pymorphy2.MorphAnalyzer()\n",
    "except Exception:\n",
    "    _MORPH = None\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bd2931-98db-42ed-9770-2a74d83c3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = ['O', 'B-BRAND', 'I-BRAND', 'B-TYPE', 'I-TYPE', 'B-VOLUME', 'I-VOLUME', 'B-PERCENT', 'I-PERCENT']\n",
    "\n",
    "# предкомпилированные регексы\n",
    "RE_NUM = re.compile(r'^\\d+([.,]\\d+)?$')\n",
    "RE_UNIT = re.compile(r'(?i)^(мл|л|г|кг|шт|уп|пак|бут|бан|таб|мг|мм|см|м)$')\n",
    "RE_NUM_UNIT_STUCK = re.compile(r'(?i)^\\d{1,5}([.,]\\d{1,2})?(мл|л|г|кг|шт|уп|пак|бут|бан|таб|мг|мм|см|м)$')\n",
    "RE_VOLUME_ANY = re.compile(r'(?i)\\b\\d{1,5}([.,]\\d{1,2})?\\s?(мл|л|г|кг|шт|уп|пак|бут|бан|таб|мг|мм|см|м)\\b')\n",
    "RE_PERCENT = re.compile(r'(?i)\\b\\d{1,2}([.,]\\d{1,2})?\\s?%')\n",
    "RE_HAS_TM = re.compile(r'[®™]')\n",
    "RE_LAT = re.compile(r'[A-Za-z]')\n",
    "RE_CYR = re.compile(r'[А-Яа-яЁё]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19be490a-ba64-4d08-84a1-a264b65d309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_script(s: str) -> bool:\n",
    "    return bool(RE_LAT.search(s) and RE_CYR.search(s))\n",
    "\n",
    "def word_shape(s: str) -> str:\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        if ch.isdigit(): out.append('d')\n",
    "        elif ch.isalpha(): out.append('X' if ch.isupper() else 'x')\n",
    "        else: out.append(ch)\n",
    "    return ''.join(out)\n",
    "\n",
    "def safe_lemma(s: str) -> str:\n",
    "    if not _MORPH:\n",
    "        return s.lower()\n",
    "    try:\n",
    "        p = _MORPH.parse(s)[0]\n",
    "        return p.normal_form\n",
    "    except Exception:\n",
    "        return s.lower()\n",
    "\n",
    "def tokenize_with_offsets(text: str) -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Простая токенизация: числа, слова (лат/кир), отдельные символы.\n",
    "    Важно сохранять оффсеты.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for m in re.finditer(r'\\d+[.,]?\\d*%?|'       # числа и проценты\n",
    "                         r'[A-Za-zА-Яа-яЁё]+'   # слова лат/кир\n",
    "                         r'|[^\\s\\w]',           # одиночные знаки\n",
    "                         text):\n",
    "        tokens.append((m.group(0), m.start(), m.end()))\n",
    "    return tokens\n",
    "\n",
    "def spans_to_bio(tokens: List[Tuple[str,int,int]], spans: List[Tuple[int,int,str]], tagset: List[str]=TAGS) -> List[str]:\n",
    "    \"\"\"\n",
    "    Преобразует char-спаны в токеновые BIO-ярлыки.\n",
    "    Жёсткое правило: на один токен один тег; если спан пересекает — считаем попадание.\n",
    "    \"\"\"\n",
    "    y = ['O'] * len(tokens)\n",
    "    # нормализуем входные спаны\n",
    "    norm = []\n",
    "    for a,b,t in spans:\n",
    "        t = t.strip()\n",
    "        if t.startswith('B-') or t.startswith('I-') or t=='O':\n",
    "            ent = t.split('-')[-1]\n",
    "        else:\n",
    "            ent = t\n",
    "        norm.append((a,b,ent))\n",
    "\n",
    "    # маркируем\n",
    "    mark = [None]*len(tokens)\n",
    "    for i,(tok, a, b) in enumerate(tokens):\n",
    "        for sa,sb,ent in norm:\n",
    "            overlap = max(0, min(b, sb) - max(a, sa))\n",
    "            if overlap>0:\n",
    "                mark[i] = ent\n",
    "                break\n",
    "\n",
    "    # в BIO\n",
    "    prev_ent = None\n",
    "    for i, ent in enumerate(mark):\n",
    "        if ent is None:\n",
    "            y[i] = 'O'\n",
    "            prev_ent = None\n",
    "        else:\n",
    "            if prev_ent == ent:\n",
    "                y[i] = f'I-{ent}'\n",
    "            else:\n",
    "                y[i] = f'B-{ent}'\n",
    "            prev_ent = ent\n",
    "    # фильтр к допустимому множеству\n",
    "    y = [t if t in tagset else 'O' for t in y]\n",
    "    return y\n",
    "\n",
    "def bio_validate(tags: List[str]) -> List[str]:\n",
    "    \"\"\"BIO-валидатор: запрещаем I-* без предшествующего B-*. Исправляем на B-* или O.\"\"\"\n",
    "    res = tags[:]\n",
    "    prev = 'O'\n",
    "    prev_ent = None\n",
    "    for i,t in enumerate(res):\n",
    "        if t=='O':\n",
    "            prev, prev_ent = 'O', None\n",
    "            continue\n",
    "        p = t.split('-',1)\n",
    "        if len(p)!=2:\n",
    "            res[i]='O'; prev,prev_ent='O',None; continue\n",
    "        bi, ent = p\n",
    "        if bi=='B':\n",
    "            prev='B'; prev_ent=ent\n",
    "        elif bi=='I':\n",
    "            if prev in ('B','I') and prev_ent==ent:\n",
    "                prev='I'\n",
    "            else:\n",
    "                # некорректный I-: превращаем в B-\n",
    "                res[i]=f'B-{ent}'\n",
    "                prev='B'; prev_ent=ent\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd76521f-cce9-4aad-bb7b-3b83190e2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lexicons(train_df: pd.DataFrame) -> Dict[str,set]:\n",
    "    \"\"\"\n",
    "    Строим простые лексиконы брендов/типов/юнитов по train.\n",
    "    Ожидает колонки: text, spans (строка со списком кортежей).\n",
    "    \"\"\"\n",
    "    brand, ttype, unit = set(), set(), set()\n",
    "    for _,row in train_df.iterrows():\n",
    "        text = str(row['text'])\n",
    "        spans = parse_spans(row['spans'])\n",
    "        toks = tokenize_with_offsets(text)\n",
    "        bio = spans_to_bio(toks, spans)\n",
    "        for (w,_,_), tag in zip(toks, bio):\n",
    "            lw = w.lower()\n",
    "            if tag.endswith('BRAND'):\n",
    "                brand.add(lw)\n",
    "            if tag.endswith('TYPE'):\n",
    "                ttype.add(lw)\n",
    "            if RE_UNIT.fullmatch(lw):\n",
    "                unit.add(lw)\n",
    "            # склейки типа 500мл учитываем как факт юнитов\n",
    "            if RE_NUM_UNIT_STUCK.fullmatch(lw):\n",
    "                unit.add(re.sub(r'^\\d+([.,]\\d+)?', '', lw))\n",
    "    return {'brand': brand, 'type': ttype, 'unit': unit}\n",
    "\n",
    "def parse_spans(spans_str: str) -> List[Tuple[int,int,str]]:\n",
    "    if not isinstance(spans_str, str):\n",
    "        return []\n",
    "    s = spans_str.strip()\n",
    "    if not s.startswith('[') or not s.endswith(']'):\n",
    "        return []\n",
    "    s = s.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        return [(int(a), int(b), str(c)) for a,b,c in val]\n",
    "    except Exception:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df16270-69e4-4416-a9d6-f0a707a2b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    use_lemma: bool = True\n",
    "    window: int = 2\n",
    "    add_context_bigrams: bool = True\n",
    "\n",
    "@dataclass\n",
    "class FeatureBuilder:\n",
    "    lexicons: Dict[str,set] = field(default_factory=lambda: {'brand':set(),'type':set(),'unit':set()})\n",
    "    cfg: FeatureConfig = field(default_factory=FeatureConfig)\n",
    "\n",
    "    def token_feats(self, sent: List[Tuple[str,int,int]], i: int) -> Dict[str,Any]:\n",
    "        w, a, b = sent[i]\n",
    "        lw = w.lower()\n",
    "        feats = {\n",
    "            'bias': 1.0,\n",
    "            'w': lw,\n",
    "            'shape': word_shape(w),\n",
    "            'is_title': w.istitle(),\n",
    "            'is_upper': w.isupper(),\n",
    "            'is_digit': w.isdigit(),\n",
    "            'len': len(w),\n",
    "            'pre1': lw[:1], 'pre2': lw[:2], 'pre3': lw[:3], 'pre4': lw[:4],\n",
    "            'suf1': lw[-1:], 'suf2': lw[-2:], 'suf3': lw[-3:], 'suf4': lw[-4:],\n",
    "            'has_tm': bool(RE_HAS_TM.search(w)),\n",
    "            'is_num_unit_stuck': bool(RE_NUM_UNIT_STUCK.fullmatch(lw)),\n",
    "            'is_volume_like': bool(RE_VOLUME_ANY.search(w)),\n",
    "            'is_percent_like': bool(RE_PERCENT.search(w)),\n",
    "            'has_mixed_script': mixed_script(w),\n",
    "            'in_brand_lex': lw in self.lexicons['brand'],\n",
    "            'in_type_lex': lw in self.lexicons['type'],\n",
    "            'in_unit_lex': lw in self.lexicons['unit'],\n",
    "            'BOS': i==0, 'EOS': i==len(sent)-1,\n",
    "        }\n",
    "        if self.cfg.use_lemma:\n",
    "            feats['lemma'] = safe_lemma(w)\n",
    "\n",
    "        # контекст\n",
    "        W = self.cfg.window\n",
    "        for off in range(1, W+1):\n",
    "            j = i-off\n",
    "            if j>=0:\n",
    "                wj = sent[j][0]\n",
    "                feats.update({\n",
    "                    f'-{off}:w': wj.lower(),\n",
    "                    f'-{off}:shape': word_shape(wj)\n",
    "                })\n",
    "        for off in range(1, W+1):\n",
    "            j = i+off\n",
    "            if j<len(sent):\n",
    "                wj = sent[j][0]\n",
    "                feats.update({\n",
    "                    f'+{off}:w': wj.lower(),\n",
    "                    f'+{off}:shape': word_shape(wj)\n",
    "                })\n",
    "\n",
    "        if self.cfg.add_context_bigrams and len(sent)>1:\n",
    "            if i>0:\n",
    "                feats['-1_bigram'] = f\"{sent[i-1][0].lower()}__{lw}\"\n",
    "            if i+1<len(sent):\n",
    "                feats['+1_bigram'] = f\"{lw}__{sent[i+1][0].lower()}\"\n",
    "        return feats\n",
    "\n",
    "    def sent2features(self, sent: List[Tuple[str,int,int]]) -> List[Dict[str,Any]]:\n",
    "        return [self.token_feats(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4621760-281b-406a-b29b-9370bd0b6ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_crf(algorithm: str='lbfgs', c1: float=0.1, c2: float=0.1, max_iter: int=200, all_transitions: bool=True) -> CRF:\n",
    "    return CRF(\n",
    "        algorithm=algorithm,\n",
    "        c1=c1, c2=c2,\n",
    "        max_iterations=max_iter,\n",
    "        all_possible_transitions=all_transitions,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "def feature_variant(builder: FeatureBuilder, kind: str) -> FeatureBuilder:\n",
    "    \"\"\"\n",
    "    A: лёгкие ортографические/регексы (use_lemma=False, window=1, no bigrams)\n",
    "    B: добавляем лексиконы (как есть)\n",
    "    C: усиливаем контекст (window=2, bigrams=True)\n",
    "    \"\"\"\n",
    "    b = FeatureBuilder(lexicons=builder.lexicons, cfg=FeatureConfig(**vars(builder.cfg)))\n",
    "    if kind=='A':\n",
    "        b.cfg.use_lemma = False\n",
    "        b.cfg.window = 1\n",
    "        b.cfg.add_context_bigrams = False\n",
    "    elif kind=='B':\n",
    "        b.cfg.use_lemma = True\n",
    "        b.cfg.window = 1\n",
    "        b.cfg.add_context_bigrams = True\n",
    "    elif kind=='C':\n",
    "        b.cfg.use_lemma = True\n",
    "        b.cfg.window = 2\n",
    "        b.cfg.add_context_bigrams = True\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05cb4397-f2f7-42a6-8f46-69e480d27beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedCRF:\n",
    "    def __init__(self, tags: List[str]=TAGS, n_splits: int=5, random_state: int=42):\n",
    "        self.tags = tags\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        # базовые фичебилдеры\n",
    "        self.base_builders = {}\n",
    "        self.base_models = {}\n",
    "        self.meta_builder = None\n",
    "        self.meta_model = None\n",
    "\n",
    "    def _probs_as_features(self, probs_seq: List[Dict[str,float]], prefix: str) -> List[Dict[str,float]]:\n",
    "        \"\"\"\n",
    "        Преобразует маргинальные вероятности CRF к фичам: {prefix:TAG -> prob}\n",
    "        \"\"\"\n",
    "        feats_seq = []\n",
    "        for p in probs_seq:\n",
    "            d = {}\n",
    "            for tag in self.tags:\n",
    "                d[f'{prefix}:{tag}'] = float(p.get(tag, 0.0))\n",
    "            feats_seq.append(d)\n",
    "        return feats_seq\n",
    "\n",
    "    def _merge_feature_dicts(self, base_feats: List[Dict[str,Any]], *others: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n",
    "        out = []\n",
    "        for i in range(len(base_feats)):\n",
    "            m = dict(base_feats[i])\n",
    "            for block in others:\n",
    "                m.update(block[i])\n",
    "            out.append(m)\n",
    "        return out\n",
    "\n",
    "    def fit(self, texts: List[str], spans_list: List[List[Tuple[int,int,str]]]):\n",
    "        # 1) токенизация и BIO\n",
    "        sents = [tokenize_with_offsets(t) for t in texts]\n",
    "        y = [spans_to_bio(s, spans) for s,spans in zip(sents, spans_list)]\n",
    "\n",
    "        # 2) лексиконы\n",
    "        tmp_df = pd.DataFrame({'text':texts, 'spans':[str(s) for s in spans_list]})\n",
    "        lex = build_lexicons(tmp_df)\n",
    "\n",
    "        # 3) билдеры A/B/C\n",
    "        base_builder = FeatureBuilder(lexicons=lex)\n",
    "        self.base_builders = {\n",
    "            'A': feature_variant(base_builder, 'A'),\n",
    "            'B': feature_variant(base_builder, 'B'),\n",
    "            'C': feature_variant(base_builder, 'C'),\n",
    "        }\n",
    "        self.meta_builder = FeatureBuilder(\n",
    "            lexicons=lex,\n",
    "            cfg=FeatureConfig(use_lemma=True, window=2, add_context_bigrams=True)\n",
    "        )\n",
    "\n",
    "        # 4) OOF предсказания для A/B/C\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        # храним по предложению: список из dict-ов на каждый токен\n",
    "        oof_probs = {k: [None] * len(sents) for k in 'ABC'}\n",
    "        self.base_models = {k: [] for k in 'ABC'}\n",
    "\n",
    "        idxs = np.arange(len(sents))\n",
    "        for fold, (tr, va) in enumerate(kf.split(idxs), start=1):\n",
    "            # подготовка фич\n",
    "            X_tr = {k: [self.base_builders[k].sent2features(sents[i]) for i in tr] for k in 'ABC'}\n",
    "            X_va = {k: [self.base_builders[k].sent2features(sents[i]) for i in va] for k in 'ABC'}\n",
    "            y_tr = [y[i] for i in tr]\n",
    "\n",
    "            # обучаем 3 базовых CRF\n",
    "            models_fold = {}\n",
    "            for k in 'ABC':\n",
    "                crf = make_crf(c1=0.05 if k=='A' else 0.1,\n",
    "                               c2=0.1 if k!='C' else 0.2,\n",
    "                               max_iter=200)\n",
    "                crf.fit(X_tr[k], y_tr)\n",
    "                models_fold[k] = crf\n",
    "\n",
    "                # маржинали на валидации:\n",
    "                # probs: List[List[Dict[tag, prob]]] — по предложениям → по токенам\n",
    "                probs = crf.predict_marginals(X_va[k])\n",
    "\n",
    "                # ВАЖНО: аккуратно разложить по исходным индексам предложений\n",
    "                for pos, i_sent in enumerate(va):\n",
    "                    oof_probs[k][i_sent] = probs[pos]\n",
    "\n",
    "            # сохраняем модели фолда (для усреднения на инференсе)\n",
    "            for k in 'ABC':\n",
    "                self.base_models[k].append(models_fold[k])\n",
    "\n",
    "        # sanity-check: все предложения должны иметь заполненные маржинали\n",
    "        for k in 'ABC':\n",
    "            for i, seq in enumerate(oof_probs[k]):\n",
    "                if seq is None:\n",
    "                    raise RuntimeError(f\"OOF probs for model {k} is None at sentence {i}. \"\n",
    "                                       \"Проверьте KFold/раскладку.\")\n",
    "\n",
    "        # 5) Формируем meta-фичи из OOF\n",
    "        X_meta = []\n",
    "        for i in range(len(sents)):\n",
    "            base_feats = self.meta_builder.sent2features(sents[i])  # базовые фичи мета-слоя\n",
    "\n",
    "            # длины должны совпасть по количеству токенов\n",
    "            n_tokens = len(base_feats)\n",
    "            if not (len(oof_probs['A'][i]) == len(oof_probs['B'][i]) == len(oof_probs['C'][i]) == n_tokens):\n",
    "                raise RuntimeError(f\"Token-length mismatch at sentence {i}: \"\n",
    "                                   f\"{len(oof_probs['A'][i])}/{len(oof_probs['B'][i])}/\"\n",
    "                                   f\"{len(oof_probs['C'][i])} vs {n_tokens}\")\n",
    "\n",
    "            pa = self._probs_as_features(oof_probs['A'][i], 'A')\n",
    "            pb = self._probs_as_features(oof_probs['B'][i], 'B')\n",
    "            pc = self._probs_as_features(oof_probs['C'][i], 'C')\n",
    "\n",
    "            mix = self._merge_feature_dicts(base_feats, pa, pb, pc)\n",
    "            X_meta.append(mix)\n",
    "    \n",
    "        # 6) Обучаем meta-CRF\n",
    "        self.meta_model = make_crf(c1=0.05, c2=0.2, max_iter=300)\n",
    "        self.meta_model.fit(X_meta, y)\n",
    "\n",
    "        self._train_sents = sents\n",
    "        self._train_y = y\n",
    "        return self\n",
    "\n",
    "    def _avg_predict_marginals(self, builder: FeatureBuilder, models: List[CRF], sent: List[Tuple[str,int,int]]) -> List[Dict[str,float]]:\n",
    "        X = builder.sent2features(sent)\n",
    "        # усредняем маржинали по k моделям (фолдам)\n",
    "        probs_list = [m.predict_marginals_single(X) for m in models]\n",
    "        out = []\n",
    "        for t in range(len(X)):\n",
    "            acc = defaultdict(float)\n",
    "            for probs in probs_list:\n",
    "                for tag, p in probs[t].items():\n",
    "                    acc[tag] += p\n",
    "            # нормализация\n",
    "            s = sum(acc.values()) or 1.0\n",
    "            out.append({k: v/s for k,v in acc.items()})\n",
    "        return out\n",
    "\n",
    "    def predict(self, texts: List[str]) -> List[List[str]]:\n",
    "        res = []\n",
    "        for text in texts:\n",
    "            sent = tokenize_with_offsets(text)\n",
    "            # базовые маржинали\n",
    "            pa = self._avg_predict_marginals(self.base_builders['A'], self.base_models['A'], sent)\n",
    "            pb = self._avg_predict_marginals(self.base_builders['B'], self.base_models['B'], sent)\n",
    "            pc = self._avg_predict_marginals(self.base_builders['C'], self.base_models['C'], sent)\n",
    "            # meta-фичи\n",
    "            base_feats = self.meta_builder.sent2features(sent)\n",
    "            fa = self._probs_as_features(pa, 'A')\n",
    "            fb = self._probs_as_features(pb, 'B')\n",
    "            fc = self._probs_as_features(pc, 'C')\n",
    "            X_meta = self._merge_feature_dicts(base_feats, fa, fb, fc)\n",
    "            # предикт\n",
    "            tags = self.meta_model.predict_single(X_meta)\n",
    "            tags = bio_validate(tags)\n",
    "            res.append(tags)\n",
    "        return res\n",
    "\n",
    "    def predict_spans(self, texts: List[str]) -> List[List[Tuple[int,int,str]]]:\n",
    "        \"\"\"\n",
    "        Возвращает списки (start, end, LABEL) по каждому тексту,\n",
    "        НЕ удаляя 'O' — т.е. 'O' тоже агрегируется в интервалы.\n",
    "        LABEL здесь «плоский» (без B-/I-): {'O','TYPE','BRAND','VOLUME','PERCENT'}.\n",
    "        \"\"\"\n",
    "        out_all = []\n",
    "        tag_seqs = self.predict(texts)  # BIO-последовательности: ['B-TYPE','I-TYPE','O',...]\n",
    "        for text, tags in zip(texts, tag_seqs):\n",
    "            sent = tokenize_with_offsets(text)  # [(tok, start, end), ...]\n",
    "            spans = []\n",
    "            cur_label = None\n",
    "            cur_start = None\n",
    "            prev_end = None\n",
    "    \n",
    "            for (w, a, b), t in zip(sent, tags):\n",
    "                # плоский лейбл для BIO: 'B-TYPE'/'I-TYPE' -> 'TYPE', 'O' -> 'O'\n",
    "                if t == 'O':\n",
    "                    flat = 'O'\n",
    "                else:\n",
    "                    bi, lab = t.split('-', 1)\n",
    "                    flat = lab\n",
    "    \n",
    "                if cur_label is None:\n",
    "                    # старт первого сегмента\n",
    "                    cur_label = flat\n",
    "                    cur_start = a\n",
    "                    prev_end = b\n",
    "                else:\n",
    "                    if flat == cur_label:\n",
    "                        # продолжаем текущий сегмент\n",
    "                        prev_end = b\n",
    "                    else:\n",
    "                        # закрываем предыдущий сегмент и открываем новый\n",
    "                        spans.append((cur_start, prev_end, cur_label))\n",
    "                        cur_label = flat\n",
    "                        cur_start = a\n",
    "                        prev_end = b\n",
    "    \n",
    "            # финализация\n",
    "            if cur_label is not None:\n",
    "                spans.append((cur_start, prev_end, cur_label))\n",
    "    \n",
    "            out_all.append(spans)\n",
    "        return out_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb4ba03-2779-463e-953d-1efdea5a79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional\n",
    "\n",
    "Span = Tuple[int, int, str]\n",
    "\n",
    "def _words_by_spaces(text: str) -> List[Tuple[int,int]]:\n",
    "    \"\"\"Разбить всю строку на «слова» как непрерывные группы непробельных символов.\"\"\"\n",
    "    words = []\n",
    "    i, n = 0, len(text)\n",
    "    while i < n:\n",
    "        while i < n and text[i].isspace():\n",
    "            i += 1\n",
    "        if i >= n: break\n",
    "        j = i\n",
    "        while j < n and not text[j].isspace():\n",
    "            j += 1\n",
    "        words.append((i, j))\n",
    "        i = j\n",
    "    return words\n",
    "\n",
    "def _label_for_word(word: Tuple[int,int], spans: List[Span]) -> Optional[str]:\n",
    "    \"\"\"Вернуть базовую метку ('TYPE','BRAND','VOLUME','PERCENT') для слова по пересечению со спанами, иначе None.\"\"\"\n",
    "    wa, wb = word\n",
    "    best_lab, best_ol = None, 0\n",
    "    for a, b, lab in spans:\n",
    "        # пересечение длинной >0\n",
    "        ol = max(0, min(wb, b) - max(wa, a))\n",
    "        if ol > best_ol:\n",
    "            best_ol = ol\n",
    "            best_lab = lab\n",
    "    return best_lab if best_ol > 0 else None\n",
    "\n",
    "def spans_to_bio_splits(text: str, spans: List[Span]) -> List[Span]:\n",
    "    \"\"\"\n",
    "    Гарантирует: одно слово -> один кортеж (start,end,label).\n",
    "    Лейблы: 'O' для слов вне сущностей; внутри сущностей — BIO по последовательным словам.\n",
    "    \"\"\"\n",
    "    # нормализуем входные метки до базовых (без 'B-','I-')\n",
    "    norm_spans: List[Span] = []\n",
    "    for a, b, lab in sorted(spans, key=lambda x: (x[0], x[1])):\n",
    "        base = lab.upper().strip()\n",
    "        if base.startswith('B-') or base.startswith('I-'):\n",
    "            base = base.split('-', 1)[1]\n",
    "        norm_spans.append((a, b, base))\n",
    "\n",
    "    words = _words_by_spaces(text)\n",
    "    result: List[Span] = []\n",
    "\n",
    "    prev_base = None\n",
    "    for (a, b) in words:\n",
    "        base = _label_for_word((a, b), norm_spans)  # None -> O\n",
    "        if base is None or base == 'O':\n",
    "            result.append((a, b, 'O'))\n",
    "            prev_base = None\n",
    "        else:\n",
    "            tag = 'B-' + base if prev_base != base else 'I-' + base\n",
    "            result.append((a, b, tag))\n",
    "            prev_base = base\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051e860e-c3a5-44c9-87a4-3ca09b2322ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_semicol_csv(path: str) -> Tuple[List[str], List[List[Tuple[int,int,str]]]]:\n",
    "    \"\"\"\n",
    "    Ожидается две колонки: text;spans\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=';', header=None, names=['text','spans'], encoding='utf-8', engine='python')\n",
    "    texts = df['text'].astype(str).tolist()\n",
    "    spans = [parse_spans(s) for s in df['spans']]\n",
    "    return texts[1:], spans[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bdb4519-60d5-4581-8717-d34a7edc0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data_base/dataset_all.csv\"   # поменяй на свой\n",
    "texts, spans = load_train_semicol_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e31dbe5-fd02-46cf-a35e-c4b7b821cb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.StackedCRF at 0x7f7e3ad2bca0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StackedCRF(n_splits=7, random_state=13)\n",
    "model.fit(texts, spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "594974c2-a23b-45b6-bd81-f243d6fc5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"unichtozhenie_petuhov_v14\"\n",
    "sub = pd.read_csv(\"submissions/sub_base.csv\", sep=';')\n",
    "preds = []\n",
    "for t in sub[\"sample\"]:\n",
    "    pred_spans = model.predict_spans([t])\n",
    "    converted = spans_to_bio_splits(t, pred_spans[0])\n",
    "    preds.append(converted)\n",
    "sub[\"annotation\"] = preds\n",
    "sub.to_csv(f\"submissions/sub_{version}.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b35b3a1-e927-48a0-b496-f3fe7cee84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d02dbaa8-13e5-4eb2-817c-564d0ef61374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_crf/StackedCRF.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, \"model_crf/StackedCRF.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69c5b8a9-c0b5-430b-8593-c88f6937ad83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.StackedCRF at 0x7f7e3ad2bca0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8512e-e2d1-436b-af3d-53691fcad15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000bbb8-8f9c-4d6e-8548-8dbe22c36e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d17290-5d8e-4741-9ad9-b8eeda51ee46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7df8c5-ad68-4b82-8ad7-86dabbd195e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf74387-256c-4697-b659-858d5b94b0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7649d83-1c37-4c12-8222-897fa353e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    \"sdafas\",\n",
    "    \"йогурт питьевой 2 % valio 500 мл\",\n",
    "    \"молоко агуша 3,2% 1л\",\n",
    "    \"сыр ламбер 200г\",\n",
    "    \"кефир 1 % бутылка 900мл\",\n",
    "    \"джин №1\",\n",
    "    \"№1 газета\",\n",
    "    \"молоко 1,5 %\",\n",
    "    \"молоко 2%\",\n",
    "    \"сок яблочный 2л\",\n",
    "    \"крекер sladoya\"\n",
    "]\n",
    "pred_tags = model.predict(test_samples)\n",
    "pred_spans = model.predict_spans(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "820858e4-0d28-4b1b-8e27-063d2108a8cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: sdafas\n",
      "SPANS: [(0, 6, 'O')]\n",
      "CONVERTED: [(0, 6, 'O')]\n",
      "\n",
      "TEXT: йогурт питьевой 2 % valio 500 мл\n",
      "SPANS: [(0, 6, 'TYPE'), (7, 15, 'O'), (16, 19, 'PERCENT'), (20, 25, 'BRAND'), (26, 32, 'VOLUME')]\n",
      "CONVERTED: [(0, 6, 'B-TYPE'), (7, 15, 'O'), (16, 17, 'B-PERCENT'), (18, 19, 'I-PERCENT'), (20, 25, 'B-BRAND'), (26, 29, 'B-VOLUME'), (30, 32, 'I-VOLUME')]\n",
      "\n",
      "TEXT: молоко агуша 3,2% 1л\n",
      "SPANS: [(0, 6, 'TYPE'), (7, 17, 'BRAND'), (18, 20, 'VOLUME')]\n",
      "CONVERTED: [(0, 6, 'B-TYPE'), (7, 12, 'B-BRAND'), (13, 17, 'I-BRAND'), (18, 20, 'B-VOLUME')]\n",
      "\n",
      "TEXT: сыр ламбер 200г\n",
      "SPANS: [(0, 10, 'TYPE'), (11, 15, 'VOLUME')]\n",
      "CONVERTED: [(0, 3, 'B-TYPE'), (4, 10, 'I-TYPE'), (11, 15, 'B-VOLUME')]\n",
      "\n",
      "TEXT: кефир 1 % бутылка 900мл\n",
      "SPANS: [(0, 5, 'TYPE'), (6, 9, 'PERCENT'), (10, 17, 'TYPE'), (18, 23, 'VOLUME')]\n",
      "CONVERTED: [(0, 5, 'B-TYPE'), (6, 7, 'B-PERCENT'), (8, 9, 'I-PERCENT'), (10, 17, 'B-TYPE'), (18, 23, 'B-VOLUME')]\n",
      "\n",
      "TEXT: джин №1\n",
      "SPANS: [(0, 4, 'TYPE'), (5, 7, 'BRAND')]\n",
      "CONVERTED: [(0, 4, 'B-TYPE'), (5, 7, 'B-BRAND')]\n",
      "\n",
      "TEXT: №1 газета\n",
      "SPANS: [(0, 2, 'BRAND'), (3, 9, 'TYPE')]\n",
      "CONVERTED: [(0, 2, 'B-BRAND'), (3, 9, 'B-TYPE')]\n",
      "\n",
      "TEXT: молоко 1,5 %\n",
      "SPANS: [(0, 6, 'TYPE'), (7, 12, 'PERCENT')]\n",
      "CONVERTED: [(0, 6, 'B-TYPE'), (7, 10, 'B-PERCENT'), (11, 12, 'I-PERCENT')]\n",
      "\n",
      "TEXT: молоко 2%\n",
      "SPANS: [(0, 6, 'TYPE'), (7, 9, 'PERCENT')]\n",
      "CONVERTED: [(0, 6, 'B-TYPE'), (7, 9, 'B-PERCENT')]\n",
      "\n",
      "TEXT: сок яблочный 2л\n",
      "SPANS: [(0, 12, 'TYPE'), (13, 15, 'VOLUME')]\n",
      "CONVERTED: [(0, 3, 'B-TYPE'), (4, 12, 'I-TYPE'), (13, 15, 'B-VOLUME')]\n",
      "\n",
      "TEXT: крекер sladoya\n",
      "SPANS: [(0, 6, 'TYPE'), (7, 14, 'BRAND')]\n",
      "CONVERTED: [(0, 6, 'B-TYPE'), (7, 14, 'B-BRAND')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t, tags, sp in zip(test_samples, pred_tags, pred_spans):\n",
    "    converted = spans_to_bio_splits(t, sp)\n",
    "    # print(\"TEXT:\", t)\n",
    "    # print(\"SPANS:\", sp)\n",
    "    # print(\"CONVERTED:\", converted)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91c64d3e-03e7-4b98-84e1-29d593595fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b062608-ff64-4ce9-bf21-3b95178af11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68040ec-0fa8-4b1d-9a67-a184145ae823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b36051a1-1762-43d0-9fcb-e0969c0b6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "\n",
    "def _is_punct(ch: str) -> bool:\n",
    "    # Любой символ категории Unicode \"P\" — пунктуация,\n",
    "    # но % исключаем из удаления\n",
    "    return unicodedata.category(ch).startswith(\"P\") and ch != \"%\"\n",
    "\n",
    "def _strip_punct(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s if not _is_punct(ch))\n",
    "\n",
    "def predict_with_punct(nlp, s: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Возвращает список [[оригинальный_фрагмент_с_пунктуацией, label], ...]\n",
    "    на основе предсказаний nlp по строке без пунктуации (кроме %).\n",
    "    \"\"\"\n",
    "    orig_tokens: List[Tuple[int,int,str]] = []\n",
    "    for m in re.finditer(r\"\\S+\", s):\n",
    "        start, end = m.span()\n",
    "        orig_tokens.append((start, end, s[start:end]))\n",
    "\n",
    "    clean_pieces = []\n",
    "    clean_spans = []\n",
    "    clean_cursor = 0\n",
    "    kept_idx = []\n",
    "\n",
    "    for i, (st, en, tok) in enumerate(orig_tokens):\n",
    "        clean_tok = _strip_punct(tok)\n",
    "        if not clean_tok:\n",
    "            continue\n",
    "        if clean_pieces:\n",
    "            clean_cursor += 1\n",
    "            clean_pieces.append(\" \")\n",
    "        clean_start = clean_cursor\n",
    "        clean_pieces.append(clean_tok)\n",
    "        clean_cursor += len(clean_tok)\n",
    "        clean_spans.append((clean_start, clean_cursor, i))\n",
    "        kept_idx.append(i)\n",
    "\n",
    "    clean_text = \"\".join(clean_pieces).lower()\n",
    "    if not clean_text.strip():\n",
    "        return []\n",
    "\n",
    "    doc = nlp(clean_text)\n",
    "\n",
    "    results: List[List[str]] = []\n",
    "    for ent in doc.ents:\n",
    "        ent_start, ent_end = ent.start_char, ent.end_char\n",
    "        covered = []\n",
    "        for cst, cen, idx in clean_spans:\n",
    "            if not (cen <= ent_start or cst >= ent_end):\n",
    "                covered.append(idx)\n",
    "        if not covered:\n",
    "            continue\n",
    "        i0, i1 = min(covered), max(covered)\n",
    "        start0 = orig_tokens[i0][0]\n",
    "        end1 = orig_tokens[i1][1]\n",
    "        orig_fragment = s[start0:end1]\n",
    "        results.append([orig_fragment, ent.label_])\n",
    "\n",
    "    return results\n",
    "\n",
    "def _split_into_tokens(text):\n",
    "    \"\"\"Разбивает текст на токены по пробелам\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    tokens = []\n",
    "    start = 0\n",
    "    for i, char in enumerate(text):\n",
    "        if char == ' ':\n",
    "            if start < i:\n",
    "                tokens.append((start, i))\n",
    "            start = i + 1\n",
    "    \n",
    "    if start < len(text):\n",
    "        tokens.append((start, len(text)))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def _tokenize_text(text):\n",
    "    \"\"\"Токенизирует текст и возвращает список токенов с их текстом и позициями\"\"\"\n",
    "    tokens = _split_into_tokens(text)\n",
    "    token_texts = []\n",
    "    for start, end in tokens:\n",
    "        token_texts.append((text[start:end].lower(), start, end))\n",
    "    return token_texts\n",
    "\n",
    "def convert_model2_to_model1(text, model2_results):\n",
    "    \"\"\"\n",
    "    Конвертирует результаты NER модели 2 в формат модели 1, используя токенизацию.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Токенизируем текст\n",
    "    text_tokens = _tokenize_text(text)\n",
    "    if not text_tokens:\n",
    "        return []\n",
    "    \n",
    "    # Если результат модели 2 пустой, все токены помечаем как 'O'\n",
    "    if not model2_results:\n",
    "        return [(start, end, 'O') for _, start, end in text_tokens]\n",
    "    \n",
    "    # Создаем список для тегов каждого токена, по умолчанию 'O'\n",
    "    tags = ['O'] * len(text_tokens)\n",
    "    \n",
    "    # Обрабатываем каждую сущность из model2_results\n",
    "    for entity in model2_results:\n",
    "        if not isinstance(entity, (list, tuple)) or len(entity) < 2:\n",
    "            continue\n",
    "            \n",
    "        entity_text, entity_type = entity[0], entity[1]\n",
    "        \n",
    "        if not isinstance(entity_text, str) or not isinstance(entity_type, str):\n",
    "            continue\n",
    "        \n",
    "        # Токенизируем сущность\n",
    "        entity_tokens = [token.lower() for token in entity_text.split()]\n",
    "        if not entity_tokens:\n",
    "            continue\n",
    "        \n",
    "        # Ищем последовательность токенов сущности в тексте\n",
    "        i = 0\n",
    "        while i <= len(text_tokens) - len(entity_tokens):\n",
    "            # Проверяем, совпадает ли последовательность токенов\n",
    "            match = True\n",
    "            for j in range(len(entity_tokens)):\n",
    "                if text_tokens[i + j][0] != entity_tokens[j]:\n",
    "                    match = False\n",
    "                    break\n",
    "            \n",
    "            if match:\n",
    "                # Нашли совпадение - размечаем токены\n",
    "                tags[i] = 'B-' + entity_type\n",
    "                for j in range(1, len(entity_tokens)):\n",
    "                    tags[i + j] = 'I-' + entity_type\n",
    "                \n",
    "                # Перескакиваем через найденную сущность\n",
    "                i += len(entity_tokens)\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    # Формируем результат\n",
    "    result = []\n",
    "    for (_, start, end), tag in zip(text_tokens, tags):\n",
    "        result.append((start, end, tag))\n",
    "    \n",
    "    return result   \n",
    "\n",
    "\n",
    "def make_submission(df, spicy_col: str):\n",
    "    formated_results = [convert_model2_to_model1(text, literal_eval(model2_results)) for (text, model2_results) in zip(df['sample'].tolist(), df[spicy_col].tolist())]\n",
    "    df['annotation'] = formated_results\n",
    "    return df[['sample', 'annotation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1df9979-8be7-449d-9a1f-3bfb604e53dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103fcc35-9bc5-4f7e-8d8f-31e4f57141e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"submissions/sub_base.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48d6b105-6e02-4400-82e0-a09812d80f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy_transformers/layers/hf_shim.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model_best = spacy.load(\"../model/model-best-best\")\n",
    "model_ner_all_word = spacy.load(\"model_ner_all_word/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3e694c6a-1bcf-42b9-b12b-5a2859153b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95f556c5-4561-4cfa-b2c4-7da4ce591c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = f\"unichtozhenie_petuhov_v15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f621a843-c7f5-40f6-b2fb-df3a9ac8631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:18<00:00, 64.00it/s] \n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "sub = pd.read_csv(\"submissions/sub_base.csv\", sep=';')\n",
    "for v in tqdm(sub['sample']):\n",
    "    pred_best = predict_with_punct(model_best, v)\n",
    "    pred_best = convert_model2_to_model1(v, pred_best)\n",
    "\n",
    "    pred_tags = model.predict([v])\n",
    "    pred_spans = model.predict_spans([v])\n",
    "    preds_crf = spans_to_bio_splits(v, pred_spans[0])\n",
    "    \n",
    "    result = preds_crf\n",
    "    if preds_crf != pred_best:\n",
    "        pred_one = predict_with_punct(model_ner_all_word, v)\n",
    "        pred_one = convert_model2_to_model1(v, pred_one)\n",
    "        \n",
    "        if pred_one == pred_best:\n",
    "            result = pred_best\n",
    "    preds.append(result)\n",
    "\n",
    "sub[\"annotation\"] = preds\n",
    "sub[[\"sample\", \"annotation\"]].to_csv(f\"submissions/sub_{version}.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ddd02354-b66c-408c-b300-361d53861f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>форма для выпечки</td>\n",
       "      <td>[(0, 5, B-TYPE), (6, 9, O), (10, 17, O)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>фарш свиной</td>\n",
       "      <td>[(0, 4, B-TYPE), (5, 11, I-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>сок ананасовый без сахара</td>\n",
       "      <td>[(0, 3, B-TYPE), (4, 14, I-TYPE), (15, 18, O),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>еринги</td>\n",
       "      <td>[(0, 6, B-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>молооко</td>\n",
       "      <td>[(0, 7, B-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>milkywa</td>\n",
       "      <td>[(0, 7, B-BRAND)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>очиститель для унитаза</td>\n",
       "      <td>[(0, 10, B-TYPE), (11, 14, O), (15, 22, O)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>арбузные</td>\n",
       "      <td>[(0, 8, B-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>кашы</td>\n",
       "      <td>[(0, 4, B-TYPE)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>рычалк</td>\n",
       "      <td>[(0, 6, B-TYPE)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sample  \\\n",
       "0             форма для выпечки   \n",
       "1                   фарш свиной   \n",
       "2     сок ананасовый без сахара   \n",
       "3                        еринги   \n",
       "4                       молооко   \n",
       "...                         ...   \n",
       "4995                    milkywa   \n",
       "4996     очиститель для унитаза   \n",
       "4997                   арбузные   \n",
       "4998                       кашы   \n",
       "4999                     рычалк   \n",
       "\n",
       "                                             annotation  \n",
       "0              [(0, 5, B-TYPE), (6, 9, O), (10, 17, O)]  \n",
       "1                     [(0, 4, B-TYPE), (5, 11, I-TYPE)]  \n",
       "2     [(0, 3, B-TYPE), (4, 14, I-TYPE), (15, 18, O),...  \n",
       "3                                      [(0, 6, B-TYPE)]  \n",
       "4                                      [(0, 7, B-TYPE)]  \n",
       "...                                                 ...  \n",
       "4995                                  [(0, 7, B-BRAND)]  \n",
       "4996        [(0, 10, B-TYPE), (11, 14, O), (15, 22, O)]  \n",
       "4997                                   [(0, 8, B-TYPE)]  \n",
       "4998                                   [(0, 4, B-TYPE)]  \n",
       "4999                                   [(0, 6, B-TYPE)]  \n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3b4ce61e-e4a9-464f-84b5-7aabd09a2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[[\"sample\", \"annotation\"]].to_csv(f\"submissions/sub_{version}.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c5ef2f6-31bc-4665-b8cf-7644d1931d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 6, 'B-TYPE')], [(0, 6, 'B-TYPE')], [(0, 4, 'B-TYPE')])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_crf, pred_best, pred_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9cce48d6-87a2-42be-9b32-ca524a7cc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[\"preds_1\"] = preds_1\n",
    "sub[\"preds_2\"] = preds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ab8d8bc5-9860-4cdf-b93d-31df87554c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6, 'B-TYPE')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26ffcd2e-4ebc-46b1-897b-e7b9d71e2ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:49<00:00, 100.82it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_1 = []\n",
    "preds_2 = []\n",
    "sub = pd.read_csv(\"submissions/sub_unichtozhenie_petuhov_v15.csv\", sep=';')\n",
    "for v in tqdm(sub['sample']):\n",
    "    pred_best = predict_with_punct(model_best, v)\n",
    "    pred_best = convert_model2_to_model1(v, pred_best)\n",
    "\n",
    "    pred_tags = model.predict([v])\n",
    "    pred_spans = model.predict_spans([v])\n",
    "    preds_crf = spans_to_bio_splits(v, pred_spans[0])\n",
    "    \n",
    "    preds_1.append(preds_crf)\n",
    "    preds_2.append(pred_best)\n",
    "\n",
    "sub[\"annotation_crf\"] = preds_1\n",
    "sub[\"annotation_spacy\"] = preds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afbb81c5-f87c-48ae-9884-d83a68397c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[sub[\"annotation_crf\"] != sub[\"annotation_spacy\"]].to_csv(\"test.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6044125-f560-4a92-be0e-d22b8ce8d4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
