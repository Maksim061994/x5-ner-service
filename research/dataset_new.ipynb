{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5bf47f6-163a-4099-ab95-4bda9422b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "from ast import literal_eval\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c059da32-8cf2-4853-917b-d664fa2936c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_punct(ch: str) -> bool:\n",
    "    # Любой символ категории Unicode \"P\" — пунктуация,\n",
    "    # но % исключаем из удаления\n",
    "    return unicodedata.category(ch).startswith(\"P\") and ch != \"%\"\n",
    "\n",
    "def _strip_punct(s: str) -> str:\n",
    "    return \"\".join(ch for ch in s if not _is_punct(ch))\n",
    "\n",
    "def predict_with_punct(nlp, s: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Возвращает список [[оригинальный_фрагмент_с_пунктуацией, label], ...]\n",
    "    на основе предсказаний nlp по строке без пунктуации (кроме %).\n",
    "    \"\"\"\n",
    "    orig_tokens: List[Tuple[int,int,str]] = []\n",
    "    for m in re.finditer(r\"\\S+\", s):\n",
    "        start, end = m.span()\n",
    "        orig_tokens.append((start, end, s[start:end]))\n",
    "\n",
    "    clean_pieces = []\n",
    "    clean_spans = []\n",
    "    clean_cursor = 0\n",
    "    kept_idx = []\n",
    "\n",
    "    for i, (st, en, tok) in enumerate(orig_tokens):\n",
    "        clean_tok = _strip_punct(tok)\n",
    "        if not clean_tok:\n",
    "            continue\n",
    "        if clean_pieces:\n",
    "            clean_cursor += 1\n",
    "            clean_pieces.append(\" \")\n",
    "        clean_start = clean_cursor\n",
    "        clean_pieces.append(clean_tok)\n",
    "        clean_cursor += len(clean_tok)\n",
    "        clean_spans.append((clean_start, clean_cursor, i))\n",
    "        kept_idx.append(i)\n",
    "\n",
    "    clean_text = \"\".join(clean_pieces).lower()\n",
    "    if not clean_text.strip():\n",
    "        return []\n",
    "\n",
    "    doc = nlp(clean_text)\n",
    "\n",
    "    results: List[List[str]] = []\n",
    "    for ent in doc.ents:\n",
    "        ent_start, ent_end = ent.start_char, ent.end_char\n",
    "        covered = []\n",
    "        for cst, cen, idx in clean_spans:\n",
    "            if not (cen <= ent_start or cst >= ent_end):\n",
    "                covered.append(idx)\n",
    "        if not covered:\n",
    "            continue\n",
    "        i0, i1 = min(covered), max(covered)\n",
    "        start0 = orig_tokens[i0][0]\n",
    "        end1 = orig_tokens[i1][1]\n",
    "        orig_fragment = s[start0:end1]\n",
    "        results.append([orig_fragment, ent.label_])\n",
    "\n",
    "    return results\n",
    "\n",
    "def _split_into_tokens(text):\n",
    "    \"\"\"Разбивает текст на токены по пробелам\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    tokens = []\n",
    "    start = 0\n",
    "    for i, char in enumerate(text):\n",
    "        if char == ' ':\n",
    "            if start < i:\n",
    "                tokens.append((start, i))\n",
    "            start = i + 1\n",
    "    \n",
    "    if start < len(text):\n",
    "        tokens.append((start, len(text)))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def _tokenize_text(text):\n",
    "    \"\"\"Токенизирует текст и возвращает список токенов с их текстом и позициями\"\"\"\n",
    "    tokens = _split_into_tokens(text)\n",
    "    token_texts = []\n",
    "    for start, end in tokens:\n",
    "        token_texts.append((text[start:end].lower(), start, end))\n",
    "    return token_texts\n",
    "\n",
    "def convert_model2_to_model1(text, model2_results):\n",
    "    \"\"\"\n",
    "    Конвертирует результаты NER модели 2 в формат модели 1, используя токенизацию.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Токенизируем текст\n",
    "    text_tokens = _tokenize_text(text)\n",
    "    if not text_tokens:\n",
    "        return []\n",
    "    \n",
    "    # Если результат модели 2 пустой, все токены помечаем как 'O'\n",
    "    if not model2_results:\n",
    "        return [(start, end, 'O') for _, start, end in text_tokens]\n",
    "    \n",
    "    # Создаем список для тегов каждого токена, по умолчанию 'O'\n",
    "    tags = ['O'] * len(text_tokens)\n",
    "    \n",
    "    # Обрабатываем каждую сущность из model2_results\n",
    "    for entity in model2_results:\n",
    "        if not isinstance(entity, (list, tuple)) or len(entity) < 2:\n",
    "            continue\n",
    "            \n",
    "        entity_text, entity_type = entity[0], entity[1]\n",
    "        \n",
    "        if not isinstance(entity_text, str) or not isinstance(entity_type, str):\n",
    "            continue\n",
    "        \n",
    "        # Токенизируем сущность\n",
    "        entity_tokens = [token.lower() for token in entity_text.split()]\n",
    "        if not entity_tokens:\n",
    "            continue\n",
    "        \n",
    "        # Ищем последовательность токенов сущности в тексте\n",
    "        i = 0\n",
    "        while i <= len(text_tokens) - len(entity_tokens):\n",
    "            # Проверяем, совпадает ли последовательность токенов\n",
    "            match = True\n",
    "            for j in range(len(entity_tokens)):\n",
    "                if text_tokens[i + j][0] != entity_tokens[j]:\n",
    "                    match = False\n",
    "                    break\n",
    "            \n",
    "            if match:\n",
    "                # Нашли совпадение - размечаем токены\n",
    "                tags[i] = 'B-' + entity_type\n",
    "                for j in range(1, len(entity_tokens)):\n",
    "                    tags[i + j] = 'I-' + entity_type\n",
    "                \n",
    "                # Перескакиваем через найденную сущность\n",
    "                i += len(entity_tokens)\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    # Формируем результат\n",
    "    result = []\n",
    "    for (_, start, end), tag in zip(text_tokens, tags):\n",
    "        result.append((start, end, tag))\n",
    "    \n",
    "    return result   \n",
    "\n",
    "\n",
    "def make_submission(df, spicy_col: str):\n",
    "    formated_results = [convert_model2_to_model1(text, literal_eval(model2_results)) for (text, model2_results) in zip(df['sample'].tolist(), df[spicy_col].tolist())]\n",
    "    df['annotation'] = formated_results\n",
    "    return df[['sample', 'annotation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4309fcb0-d63b-4bc1-a1f9-a89c9f896a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"model/model-best\")\n",
    "df = pd.read_csv(\"data_base/russian_supermarket_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0eb90b-f262-487f-8b5a-ef7ced25f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44716/44716 [05:33<00:00, 133.99it/s]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for t in tqdm(df[\"product_name\"]):\n",
    "    pred = convert_model2_to_model1(t, predict_with_punct(nlp, t))\n",
    "    res.append(pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a297335-c895-4d08-bf8e-6ddcf282167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annotation\"] = res\n",
    "df = df.rename(columns={\"product_name\": \"sample\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "166f1946-f5fe-4a07-943a-0c3e462878be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Увеличение размера выборки\n",
    "samples = []\n",
    "for i in range(df.shape[0]):\n",
    "    var1 = df[\"sample\"][i].lower().replace(\"«\", \"\").replace(\"»\", \"\").split(\", \")[0][:2]\n",
    "    var2 = df[\"sample\"][i].lower().replace(\"«\", \"\").replace(\"»\", \"\").split(\", \")[0][:5]\n",
    "    var3 = df[\"sample\"][i].lower().replace(\"«\", \"\").replace(\"»\", \"\").split(\", \")[0][:8]\n",
    "    var4 = df[\"sample\"][i].lower().replace(\"«\", \"\").replace(\"»\", \"\").split(\", \")[0][:10]\n",
    "    var5 = df[\"sample\"][i].lower().replace(\"«\", \"\").replace(\"»\", \"\").split(\", \")[0][:14]\n",
    "    var_ = df[\"sample\"][i].lower().replace(\"«\", \"\").replace(\"»\", \"\").split(\", \")[0].split(\" \")\n",
    "    var7 = \" \".join(var_[:2] + var_[-1:])\n",
    "    var8 = \" \".join(var_[-1:] + var_[:2])\n",
    "    if len(var_) > 4:\n",
    "        var9 = \" \".join(var_[:2] + var_[-2:])\n",
    "        samples.append(var9)\n",
    "        var10 = \" \".join(var_[-2:] + var_[:2])\n",
    "        samples.append(var10)\n",
    "\n",
    "    samples.append(var1)\n",
    "    samples.append(var2)\n",
    "    samples.append(var3)\n",
    "    samples.append(var4)\n",
    "    samples.append(var5)\n",
    "    samples.append(var6)\n",
    "    samples.append(var8)\n",
    "    \n",
    "samples = list(set(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d168cd21-4080-4462-bd65-5ad5d6b7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in tqdm(range(len(samples))):\n",
    "    t = samples[i]\n",
    "    pred = convert_model2_to_model1(\n",
    "        t, \n",
    "        predict_with_punct(nlp, t)\n",
    "    )\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fc604b60-c267-424b-ba92-23c343d3206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(samples, columns=[\"sample\"])\n",
    "df_res[\"annotation\"] = preds\n",
    "df_res.to_csv(\"data_base/big_big_big_dataset_spacy.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
