{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc13659-ce3c-42ea-bb56-24fbf42cb237",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "import ast"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92116ab5-dcbb-40ca-8b3e-634b417e2ef0",
   "metadata": {},
   "source": [
    "def make_new_dataset(df_raw):\n",
    "    # отфильтруем возможные строковые «шапки»/комментарии\n",
    "    df_raw = df_raw[df_raw[\"spans\"].astype(str).str.strip().str.startswith(\"[\")].reset_index(drop=True)\n",
    "    \n",
    "    rows = []\n",
    "    for sent_id, row in df_raw.iterrows():\n",
    "        text = str(row[\"text\"]) if pd.notna(row[\"text\"]) else \"\"\n",
    "        spans_str = str(row[\"spans\"])\n",
    "    \n",
    "        # парсим список спанов: [(start, end, 'B-TAG'), ...]\n",
    "        try:\n",
    "            entities = ast.literal_eval(spans_str)\n",
    "        except Exception:\n",
    "            # если строка поломана — пропускаем\n",
    "            continue\n",
    "    \n",
    "        # токенизация: последовательности непробельных символов\n",
    "        # сохраняем позицию каждого токена (start, end)\n",
    "        token_spans = [(m.group(), m.start(), m.end()) for m in re.finditer(r\"\\S+\", text)]\n",
    "    \n",
    "        # сопоставляем каждому токену метку по покрытию спаном\n",
    "        for tok, s, e in token_spans:\n",
    "            tag = \"O\"\n",
    "            for es, ee, elabel in entities:\n",
    "                # токен полностью внутри спана -> берём его метку\n",
    "                if s >= es and e <= ee:\n",
    "                    tag = elabel\n",
    "                    break\n",
    "            rows.append({\"sent_id\": sent_id, \"data\": tok, \"entities\": tag})\n",
    "\n",
    "    # итоговая таблица «как на скриншоте»\n",
    "    df = pd.DataFrame(rows, columns=[\"sent_id\", \"data\", \"entities\"])\n",
    "    return df\n",
    "\n",
    "def convert_model2_to_model1(text: str, tags: list[str]):\n",
    "    \"\"\"\n",
    "    Перевод предсказаний модели 2 (BIO список по токенам)\n",
    "    в формат модели 1 [(start, end, tag), ...].\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    tokens = text.split()\n",
    "    offset = 0\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        start = text.find(token, offset)  # ищем сдвигом, чтобы правильно найти индекс\n",
    "        end = start + len(token)\n",
    "        result.append((start, end, tag))\n",
    "        offset = end\n",
    "    return result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e5fbf0-22d4-43d8-b11d-a013972fd72e",
   "metadata": {},
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit()\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47bfdce-7cfb-4366-a374-4dd1ddedde33",
   "metadata": {},
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s['data'].values.tolist(), \n",
    "                                                           s['entities'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('sent_id').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f2f856-ab44-4d26-8b7c-30b46b899a39",
   "metadata": {},
   "source": [
    "sub = pd.read_csv(\"submissions/sub_base.csv\", sep=';')\n",
    "sub.columns = [\"text\", \"spans\"]\n",
    "crf = joblib.load('model/model_crf/crf_model.joblib')\n",
    "\n",
    "sub_df = make_new_dataset(sub)\n",
    "# пример вывода (первые 20 строк)\n",
    "print(sub_df.head(20).to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "740f66fb-0089-4c8e-8f30-1b58ce85d8e7",
   "metadata": {},
   "source": [
    "version = \"ebeqshie_v5\"\n",
    "\n",
    "sub_getter = SentenceGetter(sub_df)\n",
    "sub_sentences = sub_getter.sentences\n",
    "sub_X = [sent2features(s) for s in sub_sentences]\n",
    "y_pred_sub = crf.predict(sub_X)\n",
    "\n",
    "new_tags = []\n",
    "for i, tags in enumerate(y_pred_sub):\n",
    "    text = sub.iloc[i][\"text\"]\n",
    "    result = convert_model2_to_model1(text, tags)\n",
    "    new_tags.append(result)\n",
    "sub[\"annotation\"] = new_tags\n",
    "sub.rename(columns={\"text\": \"sample\"})[[\"sample\", \"annotation\"]].to_csv(f\"submissions/sub_{version}.csv\", sep=';', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb10cb8-e71b-41ec-a0f7-4d587ad81e1b",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
