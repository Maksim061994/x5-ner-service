{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aade69ab-d7a3-43b2-b373-8a68dd46de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('data_base/train_with_aug.csv', sep=';')\n",
    "train_df = train_data.drop_duplicates()\n",
    "\n",
    "train_data = train_data.drop_duplicates()\n",
    "\n",
    "def parse_annotation(ann):\n",
    "    return literal_eval(ann)\n",
    "\n",
    "def align_tokens_and_labels_v2(text, annotations):\n",
    "    \"\"\"\n",
    "    Улучшенная версия с обработкой граничных случаев.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    labels = ['O'] * len(tokens)\n",
    "    \n",
    "    # Сортируем аннотации по начальной позиции\n",
    "    annotations = sorted(annotations, key=lambda x: x[0])\n",
    "    \n",
    "    # Находим позиции токенов\n",
    "    current_pos = 0\n",
    "    token_positions = []\n",
    "    for token in tokens:\n",
    "        start = text.find(token, current_pos)\n",
    "        end = start + len(token)\n",
    "        current_pos = end\n",
    "        token_positions.append((start, end))\n",
    "    \n",
    "    # Сопоставляем токены с аннотациями\n",
    "    for i, (token_start, token_end) in enumerate(token_positions):\n",
    "        best_annotation = None\n",
    "        best_overlap = 0\n",
    "        \n",
    "        for start, end, label in annotations:\n",
    "            # Вычисляем перекрытие токена и аннотации\n",
    "            overlap = min(token_end, end) - max(token_start, start)\n",
    "            \n",
    "            if overlap > 0 and overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_annotation = label\n",
    "        \n",
    "        if best_annotation:\n",
    "            labels[i] = best_annotation\n",
    "    \n",
    "    return tokens, labels\n",
    "\n",
    "def make_txt(df, txt_path):\n",
    "    # Применяем функции ко всему датафрейму\n",
    "    formatted_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['sample']\n",
    "        annotations = parse_annotation(row['annotation'])\n",
    "        tokens, bio_labels = align_tokens_and_labels_v2(text, annotations)\n",
    "\n",
    "        # Формируем строки для записи в файл\n",
    "        for token, label in zip(tokens, bio_labels):\n",
    "            formatted_data.append(f\"{token} {label}\")\n",
    "        formatted_data.append('')  # Пустая строка между предложениями\n",
    "\n",
    "    # Сохраняем данные в файл train.txt в формате, ожидаемом DeepPavlov\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(formatted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d0634c-ceca-4d74-9ae2-de8085f18693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53027/2463532085.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_train_df[\"annotation\"][new_train_df[\"annotation\"].str.contains(\"'0'\")] = \"[(0, 7, 'B-TYPE'), (8, 9, 'O'), (10, 17, 'O')]\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(89278, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_df = train_df[train_df[\"sample\"].str.split(\" \").apply(lambda x: len(x) > 0)]\n",
    "new_train_df[\"len_pred\"] = new_train_df[\"annotation\"].apply(lambda x: len(literal_eval(x)))\n",
    "new_train_df = new_train_df[new_train_df.len_pred > 0]\n",
    "new_train_df[\"sample\"] = new_train_df[\"sample\"].str.lower()\n",
    "new_train_df = new_train_df.loc[new_train_df[\"sample\"].str.lower().drop_duplicates().index]\n",
    "new_train_df[\"annotation\"][new_train_df[\"annotation\"].str.contains(\"'0'\")] = \"[(0, 7, 'B-TYPE'), (8, 9, 'O'), (10, 17, 'O')]\"\n",
    "new_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2453071f-d964-4799-8928-99089539a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df_new, val_df_new = train_test_split(new_train_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7664e02-12b2-4fb1-b5cb-f122f4864ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_txt(train_df_new, 'data_base/train_all_word.txt')\n",
    "make_txt(val_df_new, 'data_base/val_all_word.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4a331e-f0dc-4eab-87ac-5f855cc40d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Auto-detected token-per-line NER format\u001B[0m\n",
      "\u001B[38;5;4mℹ Grouping every 1 sentences into a document.\u001B[0m\n",
      "\u001B[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001B[0m\n",
      "\u001B[38;5;2m✔ Generated output file (71422 documents):\n",
      "data_base/train_all_word.spacy\u001B[0m\n",
      "\u001B[38;5;4mℹ Auto-detected token-per-line NER format\u001B[0m\n",
      "\u001B[38;5;4mℹ Grouping every 1 sentences into a document.\u001B[0m\n",
      "\u001B[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001B[0m\n",
      "\u001B[38;5;2m✔ Generated output file (17856 documents):\n",
      "data_base/val_all_word.spacy\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy convert data_base/train_all_word.txt data_base/ -c ner\n",
    "!python3 -m spacy convert data_base/val_all_word.txt data_base/ -c ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca3ac0-ecb3-4641-8d98-b8d21e455c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Saving to output directory: model_ner_all_word\u001B[0m\n",
      "\u001B[38;5;4mℹ Using GPU: 1\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train config/config_transformer.cfg --output model_ner_all_word/ --paths.train data_base/train_all_word.spacy --paths.dev data_base/val_all_word.spacy --gpu-id 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9868d74-caa7-4b8f-a748-fcbbbceeb655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\n",
      "============================= Config validation =============================\u001B[0m\n",
      "\u001B[38;5;1m✘ Config validation error\u001B[0m\n",
      "disabled\tfield required\n",
      "before_creation\tfield required\n",
      "after_creation\tfield required\n",
      "after_pipeline_creation\tfield required\n",
      "{'lang': 'ru', 'pipeline': ['transformer', 'ner'], 'batch_size': 128, 'tokenizer': {'@tokenizers': 'spacy.Tokenizer.v1'}, 'vectors': {'@vectors': 'spacy.Vectors.v1'}}\n",
      "\n",
      "If your config contains missing values, you can run the 'init fill-config'\n",
      "command to fill in all the defaults, if possible:\n",
      "\n",
      "python -m spacy init fill-config config/config_transformer.cfg config/config_transformer.cfg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy debug config config/config_transformer.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fe284-5f76-4834-97bf-47a8adbfcbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
